# scaling-laws-reproduction
This repository contains reproducible experiments based on seminal scaling laws papers in deep learning, including Kaplan er al. (2020) and Hoffmann et al. (2022). It focuses on recreating core plots using synthetic and real data to explore how model performance scales with model size, compute and dataset size. 

The goal is to understand and visualize how empirical scaling laws inform the design of large language models and compute-efficient AGI systems.

- Synthetic reproduction of loss vs compute power-law curve
- (Planned) Reproduction of loss vs model size and loss vs dataset size
- Reflective notes connecting experiments to AGI alignment and compute efficiency


  ## Contents

- `notebooks/` â€” Jupyter notebooks for reproducing scaling plots
- `plots/` â€” Generated figures (e.g., loss vs compute)
- `README.md` â€” Youâ€™re here!


---

## ðŸ§  Research Motivation

This project is part of a broader AGI research journey focused on understanding scaling laws, compute efficiency, and model training optimization.

If you're also exploring AGI or efficient LLM training, feel free to fork, comment, or collaborate!

---

## ðŸ“Ž References

- Kaplan et al. (2020): https://arxiv.org/abs/2001.08361
- Hoffmann et al. (2022): https://arxiv.org/abs/2203.15556
